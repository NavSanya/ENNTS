{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c11fe3",
   "metadata": {},
   "source": [
    "# Trying to create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a605024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca12de9",
   "metadata": {},
   "source": [
    "### Single Neuron\n",
    "Each input needs a weight associated with it. Inputs are the data that we pass into the model\n",
    "to get desired outputs, while the weights are the parameters that that change inside the model during the training phase, along with biases that also change during training.\n",
    "\n",
    "Since we’re modeling a single neuron, we only have one bias, as there’s just one bias value per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf00e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc72275",
   "metadata": {},
   "source": [
    "This neuron sums each input multiplied by that input’s weight, then adds the bias. All the neuron\n",
    "does is take the fractions of inputs, where these fractions (weights) are the adjustable parameters, and adds another adjustable parameter — the bias — then outputs the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793ab56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "output = (inputs[0]*weights[0] +\n",
    "          inputs[1]*weights[1]+\n",
    "          inputs[2]*weights[2]+ bias)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f95b2",
   "metadata": {},
   "source": [
    "### Layer of Neurons\n",
    "Each neuron in a layer takes exactly the same input — the input\n",
    "given to the layer (which can be either the training data or the output from the previous layer),\n",
    "but contains its own set of weights and its own bias, producing its own unique output. The layer’s output is a set of each of these outputs — one per each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13838e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [\n",
    "          #Neuron 1\n",
    "          inputs[0]*weights1[0] +\n",
    "          inputs[1]*weights1[1]+\n",
    "          inputs[2]*weights1[2]+ \n",
    "          inputs[3]*weights1[3]+ bias1,\n",
    "          \n",
    "          #Neuron 2\n",
    "          inputs[0]*weights2[0] +\n",
    "          inputs[1]*weights2[1]+\n",
    "          inputs[2]*weights2[2]+ \n",
    "           inputs[3]*weights2[3]+ bias2,\n",
    "         \n",
    "          #Neuron 3\n",
    "          inputs[0]*weights3[0] +\n",
    "          inputs[1]*weights3[1]+\n",
    "          inputs[2]*weights3[2]+ \n",
    "          inputs[3]*weights3[3]+ bias3]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b4396",
   "metadata": {},
   "source": [
    "This is called a fully connected neural network — every neuron in the current layer has connections to every neuron from the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027a4138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights=[ [0.2, 0.8, -0.5, 1],\n",
    "         [0.5, -0.91, 0.26, -0.5], \n",
    "         [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs=[]\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    \n",
    "\n",
    "    for n_input, weight in zip (inputs, neuron_weights):\n",
    "        neuron_output +=n_input*weight\n",
    "    \n",
    "    neuron_output += neuron_bias\n",
    "    \n",
    "    layer_outputs.append(neuron_output)\n",
    "    \n",
    "print (layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99b828",
   "metadata": {},
   "source": [
    "### Single Neuron with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d312364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights= [0.2, 0.8, -0.5, 1]\n",
    "bias = 2\n",
    "\n",
    "outputs = np.dot(weights, inputs) +bias\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479589b9",
   "metadata": {},
   "source": [
    "### A layer of Neurons in NumPy\n",
    "\n",
    "##### Dot product\n",
    "`a=[1,2,3]\n",
    " b=[2,3,4]\n",
    " dot_prod = a[0]*b[0] + a[1]*b[1] + a[2]*b[2]\n",
    " output: 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c684eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights=[ [0.2, 0.8, -0.5, 1],\n",
    "         [0.5, -0.91, 0.26, -0.5], \n",
    "         [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d32404",
   "metadata": {},
   "source": [
    "### A Batch Of Data\n",
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af15651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [[1,5,6,2],\n",
    "         [3,2,1,3],\n",
    "         [5,2,1,2],\n",
    "         [6,4,8,4],\n",
    "         [2,8,5,3],\n",
    "         [1,1,9,4],\n",
    "         [6,6,0,4],\n",
    "         [8,7,6,4]]\n",
    "\n",
    "shape = [2,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18e552",
   "metadata": {},
   "source": [
    "### A Layer of Neurons and Batch of Data w/ NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb73323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1,2,3,2.5],\n",
    "          [2,5,-1,2],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights=[ [0.2, 0.8, -0.5, 1],\n",
    "         [0.5, -0.91, 0.26, -0.5], \n",
    "         [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "print (layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d18fc",
   "metadata": {},
   "source": [
    "### Adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdffaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1,2,3,2.5],\n",
    "          [2,5,-1,2],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights=[ [0.2, 0.8, -0.5, 1],\n",
    "         [0.5, -0.91, 0.26, -0.5], \n",
    "         [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1,2,-0.5]\n",
    "\n",
    "layer_outputs1 = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer_outputs2 = np.dot(layer_outputs1, np.array(weights2).T) + biases2\n",
    "\n",
    "print (layer_outputs2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80addba7",
   "metadata": {},
   "source": [
    "### Dense Layer Class/ Fully Connected NN\n",
    "We will have randon initialization of weights and biases.\n",
    "The idea here is to start a model with non-zero values small enough that they won't affect training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2eba492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-4.3361859e-05 -8.5417814e-05 -9.9373065e-05]\n",
      " [-7.3564916e-05  1.4769069e-05 -1.5383620e-04]\n",
      " [-1.3284820e-04 -2.3827555e-04 -3.0228650e-04]\n",
      " [-1.8147002e-04 -2.5475977e-04 -4.0638755e-04]]\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "            \n",
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 inputs and 3 outputs\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Forward pass of training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "#output\n",
    "print(dense1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539ad3f",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "##### Linear Activation Fynction\n",
    "y=x\n",
    "##### Sigmoid Activation Function\n",
    "y=1/(1+e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f02178",
   "metadata": {},
   "source": [
    "### ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d1f5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce9fc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d856935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333337 0.3333333  0.33333334]\n",
      " [0.33333337 0.33333328 0.33333334]\n",
      " [0.3333336  0.33333308 0.33333334]\n",
      " [0.33333364 0.33333302 0.3333333 ]]\n"
     ]
    }
   ],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims= True))\n",
    "        \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        \n",
    "'''X,y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b499b",
   "metadata": {},
   "source": [
    "### Calculating Network Error with Loss\n",
    "The loss function, also referred to as the cost function, is the\n",
    "algorithm that quantifies how wrong a model is. Loss is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.\n",
    "##### Categorical Cross-Entropy Loss\n",
    "Categorical cross-entropy is explicitly used to compare\n",
    "a “ground-truth” probability (y or “targets”) and some predicted distribution (y-hat or “predictions”), so it makes sense to use cross-entropy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_prep, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
